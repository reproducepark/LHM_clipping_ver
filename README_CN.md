# <span><img src="./assets/LHM_logo_parsing.png" height="35" style="vertical-align: top;"> - å®˜æ–¹ PyTorch å®ç°</span>

####  <p align="center"> [Lingteng Qiu<sup>*</sup>](https://lingtengqiu.github.io/), [Xiaodong Gu<sup>*</sup>](https://scholar.google.com.hk/citations?user=aJPO514AAAAJ&hl=zh-CN&oi=ao), [Peihao Li<sup>*</sup>](https://liphao99.github.io/), [Qi Zuo<sup>*</sup>](https://scholar.google.com/citations?user=UDnHe2IAAAAJ&hl=zh-CN)<br>[Weichao Shen](https://scholar.google.com/citations?user=7gTmYHkAAAAJ&hl=zh-CN), [Junfei Zhang](https://scholar.google.com/citations?user=oJjasIEAAAAJ&hl=en), [Kejie Qiu](https://sites.google.com/site/kejieqiujack/home), [Weihao Yuan](https://weihao-yuan.com/) <br>[Guanying Chen<sup>+</sup>](https://guanyingc.github.io/), [Zilong Dong<sup>+</sup>](https://baike.baidu.com/item/%E8%91%A3%E5%AD%90%E9%BE%99/62931048), [Liefeng Bo](https://scholar.google.com/citations?user=FJwtMf0AAAAJ&hl=zh-CN)</p>
###  <p align="center"> é˜¿é‡Œå·´å·´é€šä¹‰å®éªŒå®¤</p>

[![é¡¹ç›®ä¸»é¡µ](https://img.shields.io/badge/ğŸŒ-é¡¹ç›®ä¸»é¡µ-blueviolet)](https://aigc3d.github.io/projects/LHM/)
[![arXivè®ºæ–‡](https://img.shields.io/badge/ğŸ“œ-arXiv:2503-10625)](https://arxiv.org/pdf/2503.10625)
[![HuggingFace](https://img.shields.io/badge/ğŸ¤—-HuggingFace_Space-blue)](https://huggingface.co/spaces/DyrusQZ/LHM)
[![ModelScope](https://img.shields.io/badge/%20ModelScope%20-Space-blue)](https://www.modelscope.cn/studios/Damo_XR_Lab/LHM) 
[![MotionShop2](https://img.shields.io/badge/%20MotionShop2%20-Space-blue)](https://modelscope.cn/studios/Damo_XR_Lab/Motionshop2) 
[![Apacheåè®®](https://img.shields.io/badge/ğŸ“ƒ-Apache--2.0-929292)](https://www.apache.org/licenses/LICENSE-2.0)

<p align="center">
  <img src="./assets/LHM_teaser.png" heihgt="100%">
</p>

## ğŸ“¢ æœ€æ–°åŠ¨æ€
**[2025å¹´4æœˆ9æ—¥]** æˆ‘ä»¬å¼€æºäº†ç‹¬ç«‹çš„è¿åŠ¨æå–èŠ‚ç‚¹å’ŒåŠ¨ç”»æ¨ç†èŠ‚ç‚¹ï¼Œåˆ©ç”¨å·²ç»æå–å¥½çš„è¿åŠ¨å‚æ•°ï¼ŒåŠ¨ç”»æ¨ç†èŠ‚ç‚¹å¯ä»¥åœ¨20så†…äº§ç”Ÿ10sçš„è§†é¢‘!!! å‚è€ƒ[ComfyUI](https://github.com/aigc3d/LHM/tree/feat/comfyui) æ›´æ–°ä½ çš„èŠ‚ç‚¹ï¼<br>
**[2025å¹´4æœˆ9æ—¥]** æˆ‘ä»¬æä¾›äº†ä¸€å¥—è¯¦ç»†çš„æ•™ç¨‹ï¼ŒæŒ‡å¯¼å¤§å®¶å¦‚ä½•åœ¨Windowsç³»ç»Ÿä¸­å®‰è£…[LHM-ComfyUI-tutorial](https://github.com/aigc3d/LHM/blob/feat/comfyui/Windows11_install.md)!<br>
**[2025å¹´4æœˆ9æ—¥]** æˆ‘ä»¬å¼€æºäº†æ•´å¥—è§†é¢‘å¤„ç†å·¥å…·æ¥æ„å»ºæ‚¨è‡ªå·±çš„è®­ç»ƒæ•°æ® [LHM_Track](https://github.com/aigc3d/LHM_Track).<br>
**[2025å¹´4æœˆ7æ—¥]** æˆ‘ä»¬å¼€æºäº†å¦ä¸€ä¸ªé¡¹ç›® [LAM](https://github.com/aigc3d/LAM), "å•å›¾ç§’çº§æ‰“é€ è¶…å†™å®3Dæ•°å­—äºº" <br>
**[2025å¹´4æœˆ3æ—¥]** LHM-500M-HF & LHM-1B-HF æ¨¡å‹å¼€æº, æ›´é²æ£’æ›´å¿«ï¼Œç°åœ¨æ”¯æŒåŠèº«å›¾ç‰‡è¾“å…¥å•¦ï¼<br>
**[2025å¹´4æœˆ2æ—¥]** æˆ‘ä»¬æ­£å¼å‘å¸ƒäº†å®˜æ–¹çš„ ComfyUI èŠ‚ç‚¹å’Œè‡ªå®šä¹‰è§†é¢‘åŠ¨ç”»å·¥ä½œæµç¨‹ï¼ğŸ”¥ğŸ”¥ğŸ”¥ æ¬¢è¿å°è¯•ä½¿ç”¨ [ComfyUI](https://github.com/aigc3d/LHM/tree/feat/comfyui) åˆ†æ”¯ï¼<br>
**[2025å¹´3æœˆ26æ—¥]** ModelScope å¼€æºäº†ï¼Œå¿«æ¥ä½¿ç”¨æˆ‘ä»¬çš„çº¿ä¸Šèµ„æºå§ ğŸ”¥ğŸ”¥ğŸ”¥!<br>
**[2025å¹´3æœˆ20æ—¥]** å‘å¸ƒè§†é¢‘åŠ¨ä½œå¤„ç†è„šæœ¬<br>
**[2025å¹´3æœˆ19æ—¥]** æœ¬åœ°éƒ¨ç½² Gradio<br>
**[2025å¹´3æœˆ19æ—¥]** HuggingFace Demoï¼šæ›´å¿«æ›´ç¨³å®š <br>
**[2025å¹´3æœˆ15æ—¥]** æ¨ç†æ—¶é—´ä¼˜åŒ–ï¼šæé€Ÿ30% <br>
**[2025å¹´3æœˆ13æ—¥]** é¦–æ¬¡ç‰ˆæœ¬å‘å¸ƒåŒ…å«ï¼š  
âœ… æ¨ç†ä»£ç åº“  
âœ… é¢„è®­ç»ƒ LHM-0.5B æ¨¡å‹  
âœ… é¢„è®­ç»ƒ LHM-1B æ¨¡å‹  
âœ… å®æ—¶æ¸²æŸ“ç®¡çº¿  
âœ… Huggingface åœ¨çº¿æ¼”ç¤º  

### å¾…åŠæ¸…å•
- [x] æ ¸å¿ƒæ¨ç†ç®¡çº¿ (v0.1) ğŸ”¥ğŸ”¥ğŸ”¥
- [x] HuggingFace æ¼”ç¤ºé›†æˆ ğŸ¤—ğŸ¤—ğŸ¤—
- [x] ModelScope éƒ¨ç½²
- [x] åŠ¨ä½œå¤„ç†è„šæœ¬ 
- [ ] è®­ç»ƒä»£ç å‘å¸ƒ

## ğŸš€ å¿«é€Ÿå¼€å§‹

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ª [Bç«™è§†é¢‘](https://www.bilibili.com/video/BV18So4YCESk/) æ•™å¤§å®¶å¦‚ä½•ä¸€æ­¥ä¸€æ­¥çš„å®‰è£…LHM. <br>
æˆ‘ä»¬æä¾›äº†ä¸€ä¸ª [Bç«™è§†é¢‘](https://www.bilibili.com/video/BV1J9Z1Y2EiJ/) æ•™å¤§å®¶å¦‚ä½•ä¸€æ­¥ä¸€æ­¥çš„å®‰è£…LHM-ComfyUI.


### ä»Dockerä¸­æ„å»ºç¯å¢ƒ
è¯·å…ˆç¡®è®¤ä½ å®‰è£…äº†nvidia-docker
```
# CUDA 121
# step0. download docker images
wget -P ./lhm_cuda_dockers https://virutalbuy-public.oss-cn-hangzhou.aliyuncs.com/share/aigc3d/data/for_lingteng/LHM/LHM_Docker/lhm_cuda121.tar 

# step1. build from docker file
sudo docker load -i  ./lhm_cuda_dockers/lhm_cuda121.tar 

# step2. run docker_file and open the communication port 7860
sudo docker run -p 7860:7860 -v PATH/FOLDER:DOCKER_WORKSPACES -it lhm:cuda_121 /bin/bash
```

### ç¯å¢ƒé…ç½®
å…‹éš†ä»“åº“
```bash
git clone git@github.com:aigc3d/LHM.git
cd LHM
```

é€šè¿‡è„šæœ¬å®‰è£…ä¾èµ–
```
# cuda 11.8
sh ./install_cu118.sh
pip install rembg

# cuda 12.1
sh ./install_cu121.sh
pip install rembg
```
ç¯å¢ƒå·²åœ¨ python3.10ã€CUDA 11.8 å’Œ CUDA 12.1 ä¸‹æµ‹è¯•é€šè¿‡ã€‚

ä¹Ÿå¯æŒ‰æ­¥éª¤æ‰‹åŠ¨å®‰è£…ä¾èµ–ï¼Œè¯¦è§[INSTALL.md](INSTALL.md)

### æ¨¡å‹å‚æ•° 

<span style="color:red">å¦‚æœä½ æ²¡ä¸‹è½½æ¨¡å‹ï¼Œæ¨¡å‹å°†ä¼šè‡ªåŠ¨ä¸‹è½½</span>


| æ¨¡å‹ | è®­ç»ƒæ•°æ® | Transformer å±‚æ•°| ModelScope| HuggingFace| æ¨ç†æ—¶é—´ | è¦æ±‚è¾“å…¥|
| :--- | :--- | :--- | :--- | :--- | :--- |:--- |
| LHM-500M | 300K è§†é¢‘æ•°æ® + 5K 3Dæ•°æ®  | 5 | [ModelScope](https://modelscope.cn/models/Damo_XR_Lab/LHM-500M) |[huggingface](https://huggingface.co/3DAIGC/LHM-500M)| 2.01 s | å…¨èº«|
| LHM-500M-HF | 300K è§†é¢‘æ•°æ® + 5K 3Dæ•°æ® | 5 | [ModelScope](https://modelscope.cn/models/Damo_XR_Lab/LHM-500M-HF) |[huggingface](https://huggingface.co/3DAIGC/LHM-500M-HF)| 2.01 s | å…¨èº«åŠèº«|
| LHM-1.0B | 300K è§†é¢‘æ•°æ® + 5K 3Dæ•°æ® | 15 | [ModelScope](https://modelscope.cn/models/Damo_XR_Lab/LHM-1B) |[huggingface](https://huggingface.co/3DAIGC/LHM-1B)| 6.57 s | å…¨èº«|
| LHM-1B-HF | 300K è§†é¢‘æ•°æ® + 5K 3Dæ•°æ®  | 15 | [ModelScope](https://modelscope.cn/models/Damo_XR_Lab/LHM-1B-HF) |[huggingface](https://huggingface.co/3DAIGC/LHM-1B-HF)| 6.57 s |å…¨èº«åŠèº«|

æ›´å¤šæ¨¡å‹ä¿¡æ¯è§ï¼š [model_card.md](modelcard.md).


#### ä»HuggingFaceä¸‹è½½
```python
from huggingface_hub import snapshot_download 
# 500M-HF Model
model_dir = snapshot_download(repo_id='3DAIGC/LHM-500M-HF', cache_dir='./pretrained_models/huggingface')
# 500M Model
model_dir = snapshot_download(repo_id='3DAIGC/LHM-500M', cache_dir='./pretrained_models/huggingface')
# 1B Model
model_dir = snapshot_download(repo_id='3DAIGC/LHM-1B', cache_dir='./pretrained_models/huggingface')
# 1B-HF Model
model_dir = snapshot_download(repo_id='3DAIGC/LHM-1B-HF', cache_dir='./pretrained_models/huggingface')
```

#### ä»ModelScopeä¸‹è½½
```python

from modelscope import snapshot_download
# 500M-HF Model
model_dir = snapshot_download(model_id='Damo_XR_Lab/LHM-500M-HF', cache_dir='./pretrained_models')
# 500M Model
model_dir = snapshot_download(model_id='Damo_XR_Lab/LHM-500M', cache_dir='./pretrained_models')
# 1B Model
model_dir = snapshot_download(model_id='Damo_XR_Lab/LHM-1B', cache_dir='./pretrained_models')
# 1B-HF Model
model_dir = snapshot_download(model_id='Damo_XR_Lab/LHM-1B-HF', cache_dir='./pretrained_models')
```


### ä¸‹è½½å…ˆéªŒæ¨¡å‹æƒé‡
```bash
# ä¸‹è½½å…ˆéªŒæ¨¡å‹æƒé‡
wget https://virutalbuy-public.oss-cn-hangzhou.aliyuncs.com/share/aigc3d/data/LHM/LHM_prior_model.tar 
tar -xvf LHM_prior_model.tar 
```

### åŠ¨ä½œæ•°æ®å‡†å¤‡
æˆ‘ä»¬æä¾›äº†æµ‹è¯•åŠ¨ä½œç¤ºä¾‹ï¼š

```bash
# ä¸‹è½½å…ˆéªŒæ¨¡å‹æƒé‡
wget https://virutalbuy-public.oss-cn-hangzhou.aliyuncs.com/share/aigc3d/data/LHM/motion_video.tar
tar -xvf ./motion_video.tar 
```

ä¸‹è½½å®Œæˆåé¡¹ç›®ç›®å½•ç»“æ„å¦‚ä¸‹ï¼š
```bash
â”œâ”€â”€ configs
â”‚   â”œâ”€â”€ inference
â”‚   â”œâ”€â”€ accelerate-train-1gpu.yaml
â”‚   â”œâ”€â”€ accelerate-train-deepspeed.yaml
â”‚   â”œâ”€â”€ accelerate-train.yaml
â”‚   â””â”€â”€ infer-gradio.yaml
â”œâ”€â”€ engine
â”‚   â”œâ”€â”€ BiRefNet
â”‚   â”œâ”€â”€ pose_estimation
â”‚   â”œâ”€â”€ SegmentAPI
â”œâ”€â”€ example_data
â”‚   â””â”€â”€ test_data
â”œâ”€â”€ exps
â”‚   â”œâ”€â”€ releases
â”œâ”€â”€ LHM
â”‚   â”œâ”€â”€ datasets
â”‚   â”œâ”€â”€ losses
â”‚   â”œâ”€â”€ models
â”‚   â”œâ”€â”€ outputs
â”‚   â”œâ”€â”€ runners
â”‚   â”œâ”€â”€ utils
â”‚   â”œâ”€â”€ launch.py
â”œâ”€â”€ pretrained_models
â”‚   â”œâ”€â”€ dense_sample_points
â”‚   â”œâ”€â”€ gagatracker
â”‚   â”œâ”€â”€ human_model_files
â”‚   â”œâ”€â”€ sam2
â”‚   â”œâ”€â”€ sapiens
â”‚   â”œâ”€â”€ voxel_grid
â”‚   â”œâ”€â”€ arcface_resnet18.pth
â”‚   â”œâ”€â”€ BiRefNet-general-epoch_244.pth
â”œâ”€â”€ scripts
â”‚   â”œâ”€â”€ exp
â”‚   â”œâ”€â”€ convert_hf.py
â”‚   â””â”€â”€ upload_hub.py
â”œâ”€â”€ tools
â”‚   â”œâ”€â”€ metrics
â”œâ”€â”€ train_data
â”‚   â”œâ”€â”€ example_imgs
â”‚   â”œâ”€â”€ motion_video
â”œâ”€â”€ inference.sh
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
```



### ğŸ’» æœ¬åœ°éƒ¨ç½² 
æˆ‘ä»¬ç°åœ¨æ”¯æŒç”¨æˆ·è‡ªå®šä¹‰åŠ¨ä½œè¾“å…¥ï¼Œä½†æ˜¯ç”±äºåŠ¨ä½œä¼°è®¡å™¨å†…å­˜å æ¯”ï¼Œæˆ‘ä»¬LHM-500M åœ¨ç”¨æˆ·è‡ªå®šä¹‰åŠ¨ä½œè¾“å…¥gradioä¸­éœ€è¦22GB çš„å†…å­˜, ä½ ä¹Ÿå¯ä»¥æå‰å¤„ç†å¥½ï¼Œç„¶åç”¨æˆ‘ä»¬ä¹‹å‰çš„æ¥å£
```bash
# Support user motion sequence input. As the pose estimator requires some GPU memory, this Gradio application requires at least 24 GB of GPU memory to run LHM-500M.
python ./app_motion.py  
python ./app_motion.py  --model_name LHM-1B-HF

# preprocessing video sequence
python ./app.py
python ./app.py --model_name LHM-1B
```

### ğŸƒ æ¨ç†æµç¨‹
æˆ‘ä»¬ç°åœ¨æ”¯æŒåŠèº«å›¾è¾“å…¥å•¦!
<img src="./assets/half_input.gif" width="75%" height="auto"/>

```bash
# MODEL_NAME={LHM-500M, LHM-500M-HF, LHM-1B, LHM-1B-HF}
# bash ./inference.sh  LHM-500M ./train_data/example_imgs/ ./train_data/motion_video/mimo1/smplx_params
# bash ./inference.sh  LHM-1B ./train_data/example_imgs/ ./train_data/motion_video/mimo1/smplx_params
# bash ./inference.sh  LHM-500M-HF ./train_data/example_imgs/ ./train_data/motion_video/mimo1/smplx_params
# bash ./inference.sh  LHM-1B-HF ./train_data/example_imgs/ ./train_data/motion_video/mimo1/smplx_params

# export animation video
bash inference.sh ${MODEL_NAME} ${IMAGE_PATH_OR_FOLDER}  ${MOTION_SEQ}
# export mesh 
bash ./inference_mesh.sh ${MODEL_NAME} 
```
### å¤„ç†è§†é¢‘åŠ¨ä½œæ•°æ®

- ä¸‹è½½åŠ¨ä½œæå–ç›¸å…³çš„é¢„è®­ç»ƒæ¨¡å‹æƒé‡
  ```bash
  wget -P ./pretrained_models/human_model_files/pose_estimate https://virutalbuy-public.oss-cn-hangzhou.aliyuncs.com/share/aigc3d/data/LHM/yolov8x.pt
  wget -P ./pretrained_models/human_model_files/pose_estimate https://virutalbuy-public.oss-cn-hangzhou.aliyuncs.com/share/aigc3d/data/LHM/vitpose-h-wholebody.pth
  ```

- å®‰è£…é¢å¤–çš„ä¾èµ–
  ```bash
  cd ./engine/pose_estimation
  pip install mmcv==1.3.9
  pip install -v -e third-party/ViTPose
  pip install ultralytics
  ```

- è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œä»è§†é¢‘ä¸­æå–åŠ¨ä½œæ•°æ®
   ```bash
   # python ./engine/pose_estimation/video2motion.py --video_path ./train_data/demo.mp4 --output_path ./train_data/custom_motion

   python ./engine/pose_estimation/video2motion.py --video_path ${VIDEO_PATH} --output_path ${OUTPUT_PATH}

   # å¯¹äºåŠèº«è§†é¢‘ï¼Œæ¯”å¦‚./train_data/xiaoming.mp4ï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š
   python ./engine/pose_estimation/video2motion.py --video_path ${VIDEO_PATH} --output_path ${OUTPUT_PATH} --fitting_steps 100 0

   ```

- ä½¿ç”¨æå–çš„åŠ¨ä½œæ•°æ®é©±åŠ¨æ•°å­—äºº
  ```bash
  # bash ./inference.sh LHM-500M-HF ./train_data/example_imgs/ ./train_data/custom_motion/demo/smplx_params

  bash inference.sh ${MODEL_NAME} ${IMAGE_PATH_OR_FOLDER}  ${OUTPUT_PATH}/${VIDEO_NAME}/smplx_params
  ```

## è®¡ç®—æŒ‡æ ‡
æˆ‘ä»¬æä¾›äº†ç®€å•çš„æŒ‡æ ‡è®¡ç®—è„šæœ¬ï¼š
```bash
# download pretrain model into ./pretrained_models/
wget https://virutalbuy-public.oss-cn-hangzhou.aliyuncs.com/share/aigc3d/data/LHM/arcface_resnet18.pth
# Face Similarity
python ./tools/metrics/compute_facesimilarity.py -f1 ${gt_folder} -f2 ${results_folder}
# PSNR 
python ./tools/metrics/compute_psnr.py -f1 ${gt_folder} -f2 ${results_folder}
# SSIM LPIPS 
python ./tools/metrics/compute_ssim_lpips.py -f1 ${gt_folder} -f2 ${results_folder} 
```

## è‡´è°¢

æœ¬å·¥ä½œåŸºäºä»¥ä¸‹ä¼˜ç§€ç ”ç©¶æˆæœå’Œå¼€æºé¡¹ç›®æ„å»ºï¼š

- [OpenLRM](https://github.com/3DTopia/OpenLRM)
- [ExAvatar](https://github.com/mks0601/ExAvatar_RELEASE)
- [DreamGaussian](https://github.com/dreamgaussian/dreamgaussian)

æ„Ÿè°¢è¿™äº›æ°å‡ºå·¥ä½œå¯¹3Dç”Ÿæˆå’Œæ•°å­—äººé¢†åŸŸçš„é‡è¦è´¡çŒ®ã€‚
æˆ‘ä»¬è¦ç‰¹åˆ«æ„Ÿè°¢[ç«™é•¿æ¨èæ¨è](https://space.bilibili.com/175365958?spm_id_from=333.337.0.0), ä»–æ— ç§åœ°åšäº†ä¸€æ¡Bç«™è§†é¢‘æ¥æ•™å¤§å®¶å¦‚ä½•å®‰è£…LHM.


## æ›´å¤šå·¥ä½œ 
æ¬¢è¿ä½¿ç”¨æˆ‘ä»¬å›¢é˜Ÿæ›´å¤šæœ‰è¶£çš„å·¥ä½œ:
- [AniGS](https://github.com/aigc3d/AniGS)
- [LAM](https://github.com/aigc3d/LAM)

## ç‚¹èµæ›²çº¿ 

[![Star History](https://api.star-history.com/svg?repos=aigc3d/LHM)](https://star-history.com/#aigc3d/LHM&Date)

## å¼•ç”¨ 
```
@inproceedings{qiu2025LHM,
  title={LHM: Large Animatable Human Reconstruction Model from a Single Image in Seconds},
  author={Lingteng Qiu and Xiaodong Gu and Peihao Li  and Qi Zuo
     and Weichao Shen and Junfei Zhang and Kejie Qiu and Weihao Yuan
     and Guanying Chen and Zilong Dong and Liefeng Bo 
    },
  booktitle={arXiv preprint arXiv:2503.10625},
  year={2025}
}
```
